{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Twitter Data\n",
    "\n",
    "In this case study, we will explore Twitter data used in the [paper](http://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf) by Go, Bhayani and Huang (2009). Our objective is to train a classifier and then use this classifier to determine whether a tweet is positive or negative. \n",
    "\n",
    "Go, Bhayani and Huang have automatically labeled the tweets as positive, negative or neutral. Their labeling algorithm is relatively simple: If a tweet includes a positive emoticon, then they label the tweet as positive. Likewise, if the tweet has a negative emoticon, then they label the tweet as negative. If a tweet does not include any emoticon, it is labeled as neutral.\n",
    "\n",
    "From this data set, we have used only the positive and the negative tweets. These are stored in two files, \"neg_tweets.txt\" and \"pos_tweets.txt.\" Here are the first lines of these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I LOVE @Health4UandPets u guys r the best!! \"\n",
      "\n",
      "@switchfoot http://twitpic.com/2y1zl - Awww\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('pos_tweets.txt', 'r') as posfile, open('neg_tweets.txt', 'r') as negfile:\n",
    "    print posfile.readline()\n",
    "    print negfile.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets include mentions, which are marked by the '@' sign at the beginning of a word. These mentions do not help us to infer the sentiment of a tweet. Similarly, links to websites or pictures start with 'http' character array. We shall exclude both types of words. Moreover, people use shorthand letters instead of words like 'u' instead of 'you.' Therefore, we will consider words that have three or more characters. Given a tweet, we can extract the words that qualify for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOVE', 'guys', 'the', 'best!!']\n"
     ]
    }
   ],
   "source": [
    "tweet = \"I LOVE @Health4UandPets u guys r the best!! \\\"\"\n",
    "usedwords = [word for word in tweet.split() if word[0] != '@' and \\\n",
    "             len(word) >= 3 and word[0:4] != 'http']\n",
    "print usedwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better. However, we still need to strip the punctuation marks. Luckily, the `string` package includes the `translate` function that we can use. At the same time, we can convert the words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'guys', 'the', 'best']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "usedwords = [word.translate(None, string.punctuation).lower() for word in tweet.split() \\\n",
    "             if word[0] != '@' and len(word) >= 3 and word[0:4] != 'http']\n",
    "print usedwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Naive Bayes Classifier from the `nltk` package. This classifier works with a list of tokens consisting of features and labels. Here, each word that we extract from a tweet is a feature and its label will be either \"positive\" or \"negative.\" \n",
    "\n",
    "We are now ready to read the data from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cant', 'belive', 'maddy', 'dead']\n",
      "{'cant': True, 'dead': True, 'maddy': True, 'belive': True}\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "with open('neg_tweets.txt','r') as infile:\n",
    "    for line in infile:\n",
    "        usedwords = []\n",
    "        for word in line.split():\n",
    "            if word[0] != '@' and len(word) >= 3 and word[0:4] != 'http':\n",
    "                wordnp = word.translate(None, string.punctuation)\n",
    "                if not(wordnp.isdigit()):\n",
    "                    usedwords.append(wordnp.lower());\n",
    "        dictwords = dict([(word, True) for word in usedwords])\n",
    "        if len(dictwords) > 0: # We omit empty tweets\n",
    "            tweets.append((dictwords, 'negative'))\n",
    "print usedwords\n",
    "print dictwords\n",
    "\n",
    "with open('pos_tweets.txt','r') as infile:\n",
    "    for line in infile:\n",
    "        usedwords = []\n",
    "        for word in line.split():\n",
    "            if word[0] != '@' and len(word) >= 3 and word[0:4] != 'http':\n",
    "                wordnp = word.translate(None, string.punctuation)\n",
    "                if not(wordnp.isdigit()):\n",
    "                    usedwords.append(wordnp.lower());\n",
    "        dictwords = dict([(word, True) for word in usedwords])                    \n",
    "        if len(dictwords) > 0: # We omit empty tweets\n",
    "            tweets.append((dictwords, 'positive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two tweets in our desired data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oo': True, 'yeh': True, 'from': True, 'burytoes': True, 'haha': True, 'one': True, 'shud': True, 'breakfast': True, 'where': True, 'eat': True, 'day': True}\n",
      "({'awww': True}, 'negative')\n",
      "({'oo': True, 'yeh': True, 'from': True, 'burytoes': True, 'haha': True, 'one': True, 'shud': True, 'breakfast': True, 'where': True, 'eat': True, 'day': True}, 'positive')\n"
     ]
    }
   ],
   "source": [
    "print dictwords\n",
    "print tweets[0]\n",
    "print tweets[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have appended the negative tweets followed by the positive tweets, we can shuffle them before we create our data sets for testing and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'real': True, 'beijing': True, 'good': True, 'for': True, 'photo': True, 'sexy': True, 'amp': True, 'you': True, 'girl': True, 'massage': True}, 'positive')\n",
      "({'just': True, 'some': True, 'finally': True, 'finished': True, 'stuff': True, 'new': True}, 'positive')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(tweets)\n",
    "print tweets[0]\n",
    "print tweets[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to split our data set into two. We shall use a quarter of the tweets for testing and the rest will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482112\n",
      "160704\n"
     ]
    }
   ],
   "source": [
    "cutoffval = len(tweets)*3/4\n",
    "train_data = tweets[0:cutoffval];\n",
    "test_data = tweets[cutoffval:];\n",
    "print len(train_data)\n",
    "print len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import Naive Bayes Classifier as well the function to check our accuracy. Finally, we can train and test our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 11.6920001507\n",
      "Obtained Accuracy: 0.741375448029\n",
      "Most Informative Features\n",
      "                nauseous = True           negati : positi =     32.1 : 1.0\n",
      "                  ughhhh = True           negati : positi =     30.8 : 1.0\n",
      "                hayfever = True           negati : positi =     26.5 : 1.0\n",
      "               toothache = True           negati : positi =     26.4 : 1.0\n",
      "                  booooo = True           negati : positi =     25.6 : 1.0\n",
      "                     ftl = True           negati : positi =     25.5 : 1.0\n",
      "                  cancel = True           negati : positi =     24.8 : 1.0\n",
      "                 injured = True           negati : positi =     24.5 : 1.0\n",
      "                  gutted = True           negati : positi =     23.7 : 1.0\n",
      "                     bom = True           positi : negati =     23.5 : 1.0\n",
      "                stranded = True           negati : positi =     23.5 : 1.0\n",
      "                illusion = True           positi : negati =     22.5 : 1.0\n",
      "               inspiring = True           positi : negati =     22.3 : 1.0\n",
      "            followfriday = True           positi : negati =     22.0 : 1.0\n",
      "                    ache = True           negati : positi =     21.6 : 1.0\n",
      "                 unloved = True           negati : positi =     21.5 : 1.0\n",
      "                  cramps = True           negati : positi =     21.3 : 1.0\n",
      "                     sad = True           negati : positi =     21.2 : 1.0\n",
      "                    feat = True           positi : negati =     21.1 : 1.0\n",
      "               upsetting = True           negati : positi =     20.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier, util\n",
    "import time\n",
    "start = time.time()\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "print 'Elapsed time:', time.time() - start\n",
    "print 'Obtained Accuracy:', util.accuracy(classifier, test_data)\n",
    "classifier.show_most_informative_features(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few simple steps, we have obtained almost 75% accuracy. Note that we have not excluded the frequently used words (the, are, for, etc.), neither did we remove the stop words (because, but, and, etc.) It seems that with a little bit more effort, the accuracy of our analysis can be improved. Furthermore, it may be worthwile to try alternate classification methods like **Maximum Entropy** and **SVM**.\n",
    "\n",
    "It is also possible to use methods in scikit-learn with a wrapper by loading **SklearnClassifier** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 7.57499980927\n",
      "Obtained Accuracy: 0.75596127041\n",
      "Elapsed time: 6.59500002861\n",
      "Obtained Accuracy: 0.75728668857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "classify = SklearnClassifier(MultinomialNB())\n",
    "classify2 = SklearnClassifier(BernoulliNB())\n",
    "start = time.time()\n",
    "classify.train(train_data)\n",
    "print 'Elapsed time:', time.time() - start\n",
    "print 'Obtained Accuracy:', util.accuracy(classify, test_data)\n",
    "\n",
    "start = time.time()\n",
    "classify2.train(train_data)\n",
    "print 'Elapsed time:', time.time() - start\n",
    "print 'Obtained Accuracy:', util.accuracy(classify2, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}