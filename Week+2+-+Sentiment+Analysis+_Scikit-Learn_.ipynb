{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Twitter Data\n",
    "\n",
    "In this case study, we will explore Twitter data used in the [paper](http://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf) by Go, Bhayani and Huang (2009). Our objective is to train a classifier and then use this classifier to determine whether a tweet is positive or negative. \n",
    "\n",
    "Go, Bhayani and Huang have automatically labeled the tweets as positive, negative or neutral. Their labeling algorithm is relatively simple: If a tweet includes a positive emoticon, then they label the tweet as positive. Likewise, if the tweet has a negative emoticon, then they label the tweet as negative. If a tweet does not include any emoticon, it is labeled as neutral.\n",
    "\n",
    "From this data set, we have used only the positive and the negative tweets. These are stored in two files, \"neg_tweets.txt\" and \"pos_tweets.txt.\" Here are the first lines of these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I LOVE @Health4UandPets u guys r the best!! \"\r\n",
      "\n",
      "@switchfoot http://twitpic.com/2y1zl - Awww\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../data/pos_tweets.txt', 'r') as posfile, open('../data/neg_tweets.txt', 'r') as negfile:\n",
    "    print posfile.readline()\n",
    "    print negfile.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets include mentions, which are marked by the '@' sign at the beginning of a word. These mentions do not help us to infer the sentiment of a tweet. Similarly, links to websites or pictures start with 'http' character array. We shall exclude both types of words. Moreover, people use shorthand letters instead of words like 'u' instead of 'you.' Therefore, we will consider words that have three or more characters. Given a tweet, we can extract the words that qualify for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOVE', 'guys', 'the', 'best!!']\n"
     ]
    }
   ],
   "source": [
    "tweet = \"I LOVE @Health4UandPets u guys r the best!! \\\"\"\n",
    "usedwords = [word for word in tweet.split() if word[0] != '@' and \\\n",
    "             len(word) >= 3 and word[0:4] != 'http']\n",
    "print usedwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better. However, we still need to strip the punctuation marks. Luckily, the `string` package includes the `translate` function that we can use. At the same time, we can convert the words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'guys', 'the', 'best']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "usedwords = [word.translate(None, string.punctuation).lower() for word in tweet.split() \\\n",
    "             if word[0] != '@' and len(word) >= 3 and word[0:4] != 'http']\n",
    "print usedwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Bernoulii Naive Bayes Classifier from the **scikit-learn** package. This classifier works with a list of tokens consisting of features and labels. Here, each word that we extract from a tweet is a feature and its label will be either \"positive\" (1) or \"negative\" (0). \n",
    "\n",
    "We are now ready to read the data from the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "y=[]\n",
    "with open('../data/neg_tweets.txt','r') as infile:\n",
    "    for line in infile:\n",
    "        usedwords = []\n",
    "        for word in line.split():\n",
    "            if word[0] != '@' and len(word) >= 3 and word[0:4] != 'http':\n",
    "                wordnp = word.translate(None, string.punctuation)\n",
    "                if not(wordnp.isdigit()):\n",
    "                    usedwords.append(wordnp.lower());\n",
    "        dictwords = dict([(word, True) for word in usedwords])\n",
    "        if len(dictwords) > 0: # We omit empty tweets\n",
    "            tweets.append(' '.join(usedwords))\n",
    "            y.append(0)\n",
    "\n",
    "with open('../data/pos_tweets.txt','r') as infile:\n",
    "    for line in infile:\n",
    "        usedwords = []\n",
    "        for word in line.split():\n",
    "            if word[0] != '@' and len(word) >= 3 and word[0:4] != 'http':\n",
    "                wordnp = word.translate(None, string.punctuation)\n",
    "                if not(wordnp.isdigit()):\n",
    "                    usedwords.append(wordnp.lower());\n",
    "        dictwords = dict([(word, True) for word in usedwords])                    \n",
    "        if len(dictwords) > 0: # We omit empty tweets\n",
    "            tweets.append(' '.join(usedwords))\n",
    "            y.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two tweets in our desired data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awww 0\n",
      "breakfast burytoes from where oo haha shud eat burytoes one day oo yeh 1\n"
     ]
    }
   ],
   "source": [
    "print tweets[0], y[0]\n",
    "\n",
    "print tweets[-1], y[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have appended the negative tweets followed by the positive tweets, we can split our data set into two. We shall use a quarter of the tweets for testing and the rest will be used for training. **train_test_split** shuffles them before creating our data sets for testing and training.\n",
    "\n",
    "Next, we import BernoulliNB and check our accuracy and AUC after training and testing our classifiers. In order to use BernoulliNB, we must create a term-document matrix consisting of 0 and 1s by using **CountVectorizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(482112, 166370)\n",
      "BNB trained\n",
      "0.743889386699\n",
      "0.820065203043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, y,train_size=0.75)\n",
    "\n",
    "# We use stop_words = 'english' in order to remove stop words\n",
    "# In addition to that we select binary = True to create a matrix of 0 and 1s.\n",
    "cv = CountVectorizer(min_df = 1, stop_words='english', binary=True)\n",
    "cv_matrix = cv.fit_transform(X_train)\n",
    "#cv_matrix = cv_matrix.todense()\n",
    "print(np.shape(cv_matrix))\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "#This line is for training\n",
    "bnb.fit(cv_matrix,y_train)\n",
    "print('BNB trained')\n",
    "\n",
    "#In order to create test matrix we only use transform and not fit_transform\n",
    "#This is to avoid new words being added to matrix, i.e. the number of columns in train and test data sets should be equal\n",
    "cv_matrix_test = cv.transform(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "for clf in [bnb]:#,mnb,sgd]:\n",
    "    print(accuracy_score(y_test,clf.predict(cv_matrix_test)))\n",
    "    print(roc_auc_score(y_test,clf.predict_proba(cv_matrix_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few simple steps, we have obtained almost 75% accuracy. Note that we have not excluded the frequently used words (the, are, for, etc.), neither did we remove the stop words (because, but, and, etc.) It seems that with a little bit more effort, the accuracy of our analysis can be improved. Furthermore, it may be worthwile to try alternate classification methods like Random Forests and SVM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}